---
title: "Coursera Week 5 - nnCostFunction"
output:
  html_document:
    toc: true
    depth: 2    
---
[ML Home](../index.html)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}
library(tidyverse)
```

# Forward propagation and cost w/ regularization

## Glossary

Each of these variables will have a subscript, noting which NN layer it is associated with.

$\Theta$: A Theta matrix of weights to compute the inner values of the neural network. When we used a vector theta, it was noted with the lower-case theta character $\theta$

z : is the result of multiplying a data vector with a $\Theta$ matrix. A typical variable name would be "z2".

$\alpha$ : The "activation" output from a neural layer. This is always generated using a sigmoid function g() on a z value. A typical variable name would be "a2".

$\delta$ : lower-case delta is used for the "error" term in each layer.  A typical variable name would be "d2".


$\Delta$ : upper-case delta is used to hold the sum of the product of a $\delta$ value with the previous layer's $\alpha$ value. In the vectorized solution, these sums are calculated automatically though the magic of matrix algebra. A typical variable name would be "Delta2".

$\Theta$_gradient : This is the thing we're solving for, the partial derivative of theta. There is one of these variables associated with each $\Delta$. These values are returned by nnCostFunction(), so the variable names must be "Theta1_grad" and "Theta2_grad".

g() is the sigmoid function.

gâ€²() is the sigmoid gradient function. 

Tip: One handy method for excluding a column of bias units is to use the notation SomeMatrix(:,2:end). This selects all of the rows of a matrix, and omits the entire first column.

See the Appendix at the bottom of the tutorial for information on the sizes of the data objects.

A note regarding bias units, regularization, and back-propagation:

There are two methods for handing exclusion of the bias units in the Theta matrices in the back-propagation and gradient calculations. I've described only one of them here, it's the one that I understood the best. Both methods work, choose the one that makes sense to you and avoids dimension errors. It matters not a whit whether the bias unit is excluded before or after it is calculated - both methods give the same results, though the order of operations and transpositions required may be different. Those with contrary opinions are welcome to write their own tutorial. 


# tutorial for backpropagation


# References

- [Programming Tutorials](https://www.coursera.org/learn/machine-learning/discussions/weeks/4/threads/Hfo82qxTEeWjcBKYJq1ZMQ)
- [Test Case](https://www.coursera.org/learn/machine-learning/discussions/all/threads/5g8LaZTCEeW0dw6k4EUmPw)
